{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan/miniconda3/envs/ml/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/bin/sh: line 1: nvidia-smi: command not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bin/sh: line 1: nvidia-smi: command not found\n",
      "/bin/sh: line 1: nvidia-smi: command not found\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model\n",
    "- load pretrained model always\n",
    "- if we already have a fine tune saved, can apply it\n",
    "- if training model (not inference only), apply the training patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model: unsloth/Qwen2.5-3B-Instruct-unsloth-bnb-4bit\n",
      "==((====))==  Unsloth 2025.2.15: Fast Qwen2 patching. Transformers: 4.49.0.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.546 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bin/sh: line 1: nvidia-smi: command not found\n",
      "/bin/sh: line 1: nvidia-smi: command not found\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 8192\n",
    "model_names = [\n",
    "    'unsloth/Qwen2.5-0.5B-bnb-4bit',\n",
    "    'unsloth/Qwen2.5-1.5B-bnb-4bit',\n",
    "    'unsloth/Qwen2.5-3B-Instruct-unsloth-bnb-4bit',\n",
    "]\n",
    "model_name = model_names[2]\n",
    "outpath = 'posttrained/qwen3b' # used for load/saving\n",
    "print(f'Base Model: {model_name}')\n",
    "load_in_4bit = True\n",
    "dtype =  None  # none for autodetection\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit\n",
    ")\n",
    "\n",
    "# some tokens that we might use later\n",
    "IM_START = '<|im_start|>'\n",
    "IM_END = '<|im_end|>'\n",
    "EOS_TOKEN = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional step if you are resuming from a saved lora\n",
    "model.load_adapter(outpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test inference, aka model output\n",
    "from transformers import TextIteratorStreamer\n",
    "from threading import Thread\n",
    "import textwrap\n",
    "\n",
    "\n",
    "def run_inference(model, tokenizer, input=None, max_tokens=256):\n",
    "    # inference\n",
    "    FastLanguageModel.for_inference(model)  # required toggle for unsloth model\n",
    "    input_str = input or \"First Citizen:\"\n",
    "    inputs = tokenizer(\n",
    "        [input_str]*1,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    text_streamer = TextIteratorStreamer(tokenizer)\n",
    "    generation_kwargs = dict(\n",
    "        inputs,\n",
    "        streamer=text_streamer,\n",
    "        max_new_tokens=max_tokens,\n",
    "        use_cache=True,\n",
    "    )\n",
    "    thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
    "    thread.start()\n",
    "\n",
    "    # unsloth printing for narrow view\n",
    "    max_print_width = 100\n",
    "    length = 0\n",
    "    for i, new_text in enumerate(text_streamer):\n",
    "        if i == 0:\n",
    "            wrapped_text = textwrap.wrap(new_text, width=max_print_width)\n",
    "            length = len(wrapped_text[-1])\n",
    "            wrapped_text = \"\\n\".join(wrapped_text)\n",
    "            print(wrapped_text, end=\"\")\n",
    "        else:\n",
    "            length += len(new_text)\n",
    "            if length >= max_print_width:\n",
    "                length = 0\n",
    "                print()\n",
    "            print(new_text, end=\"\")\n",
    "\n",
    "in1=f'''{IM_START}user\n",
    "Continue the shakespearean dialogue below.\n",
    "First Citizen:\n",
    "Before we proceed any further, hear me speak.{IM_END}\n",
    "{IM_START}assistant'''\n",
    "\n",
    "# run_inference(model, tokenizer, input=in1)\n",
    "# _ = model.train()  # switch back to training mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "- patch model for training\n",
    "- load our data in\n",
    "    - provided is a sample shakespeare dataset\n",
    "    - the goal is to create a dataset with our Castle Agent (reward based on some accuracy metric)\n",
    "- create trainer\n",
    "- run trainer (wrapped with stats pre/post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2250 250\n",
      "{'text': 'Once upon a time, there was a small girl named Susie. Susie loved to go exploring in the big backyard of her house. One day, Susie went outside with her teddy bear. As she was running around, she saw something hidden behind a bush. She went near it, and when she peeked behind the bush, she found a kitty! Susie was so excited to see the kitty that she ran over to it. \\n\\nSusie said to the kitty, \"Hi kitty, why are you here all alone?\" \\n\\nThe kitty meowed and then said, \"I\\'m lost. I don\\'t know how to get back home.\" \\n\\nSusie felt so sorry for the kitty and she wanted to help him. She said, \"Don\\'t worry, kitty. I can help you find your way home! Let\\'s go visit and see.\" \\n\\nSo Susie and the kitty started walking to find the kitty\\'s home. After a while, they ran into some trouble. There was a big angry dog that blocked their way. Susie and the kitty were scared, but Susie kept her head and said, \"Don\\'t worry, kitty. We can find another way.\" So they both ran away and found a hidden path to get to the kitty\\'s home.\\n\\nFinally, they arrived at the kitty\\'s house. They said goodbye to each<|im_end|>'}\n"
     ]
    }
   ],
   "source": [
    "# explore datasets\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset('roneneldan/TinyStories', split=\"train[:2500]\")\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "def format_prompts_func(examples):\n",
    "    return {'text': [i + EOS_TOKEN for i in examples['text']]}\n",
    "dataset = dataset.map(format_prompts_func, batched=True)\n",
    "split_data = dataset.train_test_split(test_size=0.1, seed=12)\n",
    "train_dataset, val_dataset = split_data['train'], split_data['test']\n",
    "print(len(train_dataset), len(val_dataset))\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Offloading input_embeddings to disk to save VRAM\n",
      "Unsloth: Offloading output_embeddings to disk to save VRAM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.2.15 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Training embed_tokens in mixed precision to save VRAM\n",
      "Unsloth: Training lm_head in mixed precision to save VRAM\n"
     ]
    }
   ],
   "source": [
    "# patch the quantized model to allow for training\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=128, # 32, 64, 128\n",
    "    lora_alpha=256, # double r\n",
    "    target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj',\n",
    "                    'gate_proj', 'up_proj', 'down_proj',\n",
    "\n",
    "                    'embed_tokens', 'lm_head'],  # last two for \"continual pretraining\" of ood data?\n",
    "    lora_dropout=0.,\n",
    "    bias='none',\n",
    "    use_gradient_checkpointing='unsloth',\n",
    "    random_state=3407,\n",
    "    use_rslora=True,  # \"rank stablized\" lora\n",
    "    loftq_config=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan/miniconda3/envs/ml/lib/python3.12/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/dan/miniconda3/envs/ml/lib/python3.12/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Applying chat template to train dataset (num_proc=8): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2250/2250 [00:00<00:00, 2593.67 examples/s]\n",
      "Tokenizing train dataset (num_proc=8): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2250/2250 [00:00<00:00, 2392.39 examples/s]\n",
      "Truncating train dataset (num_proc=8): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2250/2250 [00:00<00:00, 16270.89 examples/s]\n",
      "Applying chat template to eval dataset (num_proc=8): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:00<00:00, 303.36 examples/s]\n",
      "Tokenizing eval dataset (num_proc=8): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:00<00:00, 296.01 examples/s]\n",
      "Truncating eval dataset (num_proc=8): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [00:00<00:00, 2233.94 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# implement trainer\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "from unsloth import UnslothTrainer, UnslothTrainingArguments\n",
    "\n",
    "trainer = UnslothTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    dataset_text_field='text',\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=8, # i believe this is num gpus?\n",
    "    args=UnslothTrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=8,\n",
    "\n",
    "        # warmup_ratio=0.1,\n",
    "        num_train_epochs=3,\n",
    "\n",
    "        learning_rate=5e-5,\n",
    "        embedding_learning_rate=5e-6,\n",
    "\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        optim='adamw_8bit',\n",
    "        weight_decay=0.0,\n",
    "        lr_scheduler_type='cosine',\n",
    "        seed=3407,\n",
    "\n",
    "        # evaluation added\n",
    "        evaluation_strategy='epoch',\n",
    "        save_strategy='epoch',\n",
    "        logging_steps=1,\n",
    "\n",
    "        output_dir='outputs',\n",
    "        report_to='none'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 4090. Max memory = 23.546 GB.\n",
      "3.744 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "# BEFORE RUN: gpu stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / (1024**3), 3)\n",
    "max_memory = round(gpu_stats.total_memory/(1024**3),3)\n",
    "print(f'GPU = {gpu_stats.name}. Max memory = {max_memory} GB.')\n",
    "print(f'{start_gpu_memory} GB of memory reserved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 2,250 | Num Epochs = 3\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 8\n",
      "\\        /    Total batch size = 16 | Total steps = 420\n",
      " \"-____-\"     Number of trainable parameters = 861,798,400\n",
      "/bin/sh: line 1: nvidia-smi: command not found\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='420' max='420' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [420/420 10:53, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.518800</td>\n",
       "      <td>1.382645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.878200</td>\n",
       "      <td>1.445567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.317300</td>\n",
       "      <td>1.826757</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but Qwen2ForCausalLM does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    }
   ],
   "source": [
    "# do the training\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "658.1967 seconds used for training.\n",
      "10.97 minutes used for training.\n",
      "Peak reserved memory = 10.049 GB.\n",
      "Peak reserved memory for training = 6.305 GB.\n",
      "Peak reserved memory % of max memory = 42.678 %.\n",
      "Peak reserved memory for training % of max memory = 26.777 %.\n"
     ]
    }
   ],
   "source": [
    "#@title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user Continue the shakespearean dialogue below. First Citizen: What, sayeth I, have you\n",
      "done!?<|im_end|>\n",
      "<|im_start|>assistant: I have made a cake! It is so yummy and sweet. I am very proud of it. \n",
      "First Citizen: That's great! \n",
      "What a brave thing to do! Do you want me to help you display it? \n",
      "<|im_end|>"
     ]
    }
   ],
   "source": [
    "# run some inference\n",
    "input_text = '''<|im_start|>user Continue the shakespearean dialogue below.\n",
    "First Citizen: What, sayeth I, have you done!?<|im_end|>\n",
    "<|im_start|>assistant'''\n",
    "run_inference(model, tokenizer, input=input_text)\n",
    "_ = model.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
